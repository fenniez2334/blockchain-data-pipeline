#!/usr/bin/env python
# coding: utf-8

import argparse

import pyspark
from pyspark.sql import SparkSession

parser = argparse.ArgumentParser()

parser.add_argument('--bucket', required=True)
parser.add_argument('--input_blocks', required=True)
parser.add_argument('--input_transactions', required=True)
parser.add_argument('--output1', required=True)
parser.add_argument('--output2', required=True)

args = parser.parse_args()

bucket = args.bucket
input_blocks = args.input_blocks
input_transactions = args.input_transactions
output1 = args.output1
output2 = args.output2

spark = SparkSession.builder \
    .appName('test') \
    .getOrCreate()

# update the temp GCS bucket configuration with the bucket name generated by the Dataproc cluster
spark.conf.set('temporaryGcsBucket', bucket)

# reading data as spark dataframes with previously 

df_blocks = spark.read\
                    .parquet(input_blocks)

df_transactions = spark.read\
                    .parquet(input_transactions)

df_blocks.registerTempTable('blocks')
df_transactions.registerTempTable('transactions')

df_blocks.write.format('bigquery') \
    .option('table', output1) \
    .mode('overwrite') \
    .save()

df_transactions.write.format('bigquery') \
    .option('table', output2) \
    .mode('overwrite') \
    .save()
    